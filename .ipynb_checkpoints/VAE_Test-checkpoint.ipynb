{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c201a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from clearml import Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5b296f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=373cf2b128b447c6a8512b42d57625c4\n",
      "ClearML results page: https://tks-zx.fzi.de:8080/projects/013741c08abc4e298d7346004c9c73dd/experiments/373cf2b128b447c6a8512b42d57625c4/output/log\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"bogdoll/Anomaly_detection_Moritz\", task_name=\"example_run\", reuse_last_task_id=False)\n",
    "task.set_base_docker(\n",
    "            \"tks-zx-01.fzi.de/autonomous-agents/core-carla:21.10\",\n",
    "            docker_setup_bash_script=\"apt-get update && apt-get install -y python3-opencv\",\n",
    "            docker_arguments=\"-e NVIDIA_DRIVER_CAPABILITIES=all\",  # --ipc=host\",\n",
    "        )\n",
    "# PyTorch fix for version 1.10, see https://github.com/pytorch/pytorch/pull/69904\n",
    "# task.add_requirements(\n",
    "#     package_name=\"setuptools\",\n",
    "#     package_version=\"59.5.0\",\n",
    "# )\n",
    "# task.add_requirements(\n",
    "#     package_name=\"moviepy\",\n",
    "#     package_version=\"1.0.3\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba17dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.FashionMNIST(root='fashion_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = datasets.FashionMNIST(root='fashion_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20b914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:19:14,767 - clearml.Task - INFO - No repository found, storing script code instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/fzi-is789/ipykernel_1191975/537219179.py:7: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "/tmp/fzi-is789/ipykernel_1191975/537219179.py:7: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## exclude label\n",
    "def discardLabels(data):\n",
    "    new_data = []\n",
    "    for img, _ in data:\n",
    "        new_data.append(img)\n",
    "        \n",
    "    return np.array(new_data)\n",
    "\n",
    "mnist_train = discardLabels(mnist_train)\n",
    "mnist_test = discardLabels(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddedd414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## apply noise\n",
    "noise_set = []\n",
    "\n",
    "for img in mnist_test:\n",
    "    pic = img.numpy().copy()\n",
    "    pic[:,10:15,:] = 0.0\n",
    "    noise_set.append(pic)\n",
    "\n",
    "noise_set = np.array(noise_set)\n",
    "noise_set = torch.as_tensor(noise_set)\n",
    "noise_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4cee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdcbb2013a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## show noisy image\n",
    "\n",
    "img = noise_set[10]\n",
    "img = img.squeeze()\n",
    "img = img.numpy()\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c07f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdcc705be20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist_test[1]\n",
    "img = img.squeeze()\n",
    "print(img.shape)\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ddb5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82e994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "dataloaders[\"test\"] = DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "dataloaders[\"noise\"] = DataLoader(dataset=noise_set,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e4019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdcc6f83040>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = next(iter(dataloaders[\"test\"]))\n",
    "plt.imshow(images[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d28fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, imgChannels=1, featureDim=128*20*20, zDim=20):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(imgChannels, 16, 3)\n",
    "        self.encConv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.encConv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.encConv4 = nn.Conv2d(64, 128, 3)\n",
    "#         self.encConv5 = nn.Conv2d(128, 256, 3)\n",
    "#         self.encConv6 = nn.Conv2d(256, 512, 3)\n",
    "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "        self.encFC2 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "#         self.decConv1 = nn.ConvTranspose2d(512, 256, 5)\n",
    "#         self.decConv2 = nn.ConvTranspose2d(256, 128, 3)\n",
    "        self.decConv3 = nn.ConvTranspose2d(128, 64, 3)\n",
    "        self.decConv4 = nn.ConvTranspose2d(64, 32, 3)\n",
    "        self.decConv5 = nn.ConvTranspose2d(32, 16, 3)\n",
    "        self.decConv6 = nn.ConvTranspose2d(16, imgChannels, 3)\n",
    "        \n",
    "        self.flatten = 0\n",
    "\n",
    "    def encoder(self, x):\n",
    "\n",
    "        # Input is fed into 2 convolutional layers sequentially\n",
    "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
    "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
    "        x = F.relu(self.encConv1(x))\n",
    "#         print(x.size())\n",
    "        x = F.relu(self.encConv2(x))\n",
    "#         print(x.size())\n",
    "        x = F.relu(self.encConv3(x))\n",
    "#         print(x.size())\n",
    "        x = F.relu(self.encConv4(x))\n",
    "#         print(x.size())\n",
    "#         x = F.relu(self.encConv5(x))\n",
    "#         print(x.size())\n",
    "# #         x = F.relu(self.encConv6(x))\n",
    "#         print(x.size())\n",
    "        self.flatten = x.size(1) * x.size(2) * x.size(3)\n",
    "#         print(self.flatten)\n",
    "        x = x.view(-1, self.flatten)\n",
    "        mu = self.encFC1(x)\n",
    "        logVar = self.encFC2(x)\n",
    "        return mu, logVar\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "\n",
    "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "        std = torch.exp(logVar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
    "        # The generated output is the same size of the original input\n",
    "        x = F.relu(self.decFC1(z))\n",
    "        x = x.view(-1, 128, 20, 20)\n",
    "#         x = F.relu(self.decConv1(x))\n",
    "#         x = F.relu(self.decConv2(x))\n",
    "        x = F.relu(self.decConv3(x))\n",
    "        x = F.relu(self.decConv4(x))\n",
    "        x = F.relu(self.decConv5(x))\n",
    "        x = torch.sigmoid(self.decConv6(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
    "        # output, mu, and logVar are returned for loss computation\n",
    "        mu, logVar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185297f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as  F\n",
    "def loss_fn(x, recon_x, mu, log_var):\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, 784), x.view(-1, 784), reduction = \"sum\")\n",
    "    KLD_loss = 0.5 * torch.sum(mu.pow(2) + log_var.exp() - 1 - log_var)\n",
    "    return Recon_loss + KLD_loss\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ce6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encConv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (encConv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (encConv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (encConv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (encFC1): Linear(in_features=51200, out_features=20, bias=True)\n",
      "  (encFC2): Linear(in_features=51200, out_features=20, bias=True)\n",
      "  (decFC1): Linear(in_features=20, out_features=51200, bias=True)\n",
      "  (decConv3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (decConv4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (decConv5): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (decConv6): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa7fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:19:30,830 - clearml - WARNING - Switching to remote execution, output log page https://tks-zx.fzi.de:8080/projects/013741c08abc4e298d7346004c9c73dd/experiments/373cf2b128b447c6a8512b42d57625c4/output/log\n"
     ]
    }
   ],
   "source": [
    "epoch = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for e in range(1, epoch+1):\n",
    "    if epoch > 1:\n",
    "        task.execute_remotely('docker', clone=False, exit_process=True)\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for x in dataloaders[\"train\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon, mu, log_var = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(x, x_recon, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    for x in dataloaders[\"test\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon, mu, log_var = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(x, x_recon, mu, log_var)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "    val_loss /= len(dataloaders[\"test\"].dataset)\n",
    "\n",
    "    print(f\"Epoch {e} | Loss: {train_loss} | V_Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a281d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printReconError(img_in, img_out, threshold=None):\n",
    "    errorMatrix = np.absolute(img_in - img_out)\n",
    "    if not threshold == None:\n",
    "        errorMatrix[errorMatrix < threshold] = 0.0\n",
    "    errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1])\n",
    "    print(f\"MAE: {errorAvg}\")\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,15))\n",
    "    \n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.imshow(img_in, cmap=\"gray\")\n",
    "    ax2.set_title(\"Recreation\")\n",
    "    ax2.imshow(img_out, cmap=\"gray\")\n",
    "    ax3.set_title(\"ErrorMap\")\n",
    "    ax3.imshow(errorMatrix, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs in random.sample(list(dataloaders[\"noise\"]), 1):\n",
    "        imgs = imgs.to(device)\n",
    "#         plt.subplot(121)\n",
    "        img = imgs[0].cpu().numpy()\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "#         plt.imshow(img, cmap=\"gray\")\n",
    "        \n",
    "        out, mu, logVAR = model(imgs)\n",
    "#         plt.subplot(122)\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (1,2,0))\n",
    "#         plt.imshow(out, cmap=\"gray\")\n",
    "        \n",
    "        printReconError(img, out, 0.4)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35169083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "tmp.append(img)\n",
    "tmp = np.array(tmp)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad867d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs in random.sample(list(dataloaders[\"test\"]), 1):\n",
    "#         imgs, _ = data\n",
    "        imgs = imgs.to(device)\n",
    "        print(imgs.shape)\n",
    "#         img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        plt.subplot(121)\n",
    "        img = imgs[0].cpu().numpy()\n",
    "        img = np.transpose(img, (1,2,0))\n",
    "        print(img.dtype)\n",
    "        print(img.shape)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        out, mu, logVAR = model(imgs)\n",
    "#         outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        plt.subplot(122)\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (1,2,0))\n",
    "        print(out.shape)\n",
    "        plt.imshow(out, cmap=\"gray\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba07316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly",
   "language": "python",
   "name": "anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
