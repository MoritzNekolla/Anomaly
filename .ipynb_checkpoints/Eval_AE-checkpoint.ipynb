{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c562d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "\n",
    "from Image_Sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"clearML\"\n",
    "PATH = \"model.pt\"\n",
    "# IMG_TRAIN = \"/disk/vanishing_data/is789/anomaly_samples/train_set/\"\n",
    "# IMG_TEST = \"/disk/vanishing_data/is789/anomaly_samples/40test/\"\n",
    "\n",
    "parameters = {\n",
    "    \"epoch\" : 1000,\n",
    "    \"batch_size\" : 10,\n",
    "    \"imgSize\": 512,\n",
    "    \"zDim\": 512,\n",
    "    \"learning_rate\" : 1e-05,\n",
    "#     \"layers\" : [64, 128, 256, 256, 512, 512, 940],\n",
    "    \"layers\" : [64, 120, 240, 480, 512],\n",
    "    \"layers_out\" : [512,256,128,64],\n",
    "    \"reduce_threshold\" : [0.6,0.8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746b689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, imgChannels=3, imgSize=parameters[\"imgSize\"], zDim=parameters[\"zDim\"]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        \n",
    "        stride=[1,2,2,2,2]\n",
    "        out_stride=[2,2,2,2,1]\n",
    "#         in_stride=[1,2,2,2,2]\n",
    "#         out_stride=[1,2,2,2,1]\n",
    "        in_padding=[3,0,0,0,0]\n",
    "        in_trans_padding=[0,0,1,0,0]\n",
    "        out_padding=[0,0,1,0,0]\n",
    "        kernel=[7,3,3,3,3]\n",
    "        kernel_out=[3,3,4,4,1]\n",
    "#         layers=[128, 128, 128, 256, 256]\n",
    "        layers=parameters[\"layers\"]\n",
    "        layers_out = parameters[\"layers_out\"]\n",
    "#         layers=[32, 64, 64, 128, 128]\n",
    "#         layers=[64, 128, 128, 128, 256]\n",
    "        \n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(in_channels=imgChannels, out_channels=layers[0], kernel_size=kernel[0], stride=stride[0], padding=in_padding[0])\n",
    "        self.encBn1 = nn.BatchNorm2d(layers[0])\n",
    "        self.encConv2 = nn.Conv2d(in_channels=layers[0], out_channels=layers[1], kernel_size=kernel[1], stride=stride[1], padding=in_padding[1])\n",
    "        self.encBn2 = nn.BatchNorm2d(layers[1])\n",
    "        self.encConv3 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2], kernel_size=kernel[2], stride=stride[2], padding=in_padding[2])\n",
    "        self.encBn3 = nn.BatchNorm2d(layers[2])\n",
    "        self.encConv4 = nn.Conv2d(in_channels=layers[2], out_channels=layers[3], kernel_size=kernel[3], stride=stride[3], padding=in_padding[3])\n",
    "        self.encBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.encConv5 = nn.Conv2d(in_channels=layers[3], out_channels=layers[4], kernel_size=kernel[4], stride=stride[4], padding=in_padding[4])\n",
    "        self.encBn5 = nn.BatchNorm2d(layers[4])\n",
    "#         self.encConv6 = nn.Conv2d(in_channels=layers[4], out_channels=layers[5], kernel_size=kernel[5], stride=stride[5], padding=in_padding[5])\n",
    "#         self.encBn6 = nn.BatchNorm2d(layers[5])\n",
    "#         self.encConv7 = nn.Conv2d(in_channels=layers[5], out_channels=layers[6], kernel_size=kernel[6], stride=stride[6], padding=in_padding[6])\n",
    "#         self.encBn7 = nn.BatchNorm2d(layers[6])\n",
    "        \n",
    "        encoderDims = self.calcEncoderDims(len(layers), imgSize, kernel, in_padding, stride)\n",
    "#         featureDim = layers[-1] * encoderDims[-1] * encoderDims[-1]\n",
    "#         self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "# #         Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "#         self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "#         self.decBn1 = nn.BatchNorm1d(featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(in_channels=layers[4], out_channels=layers_out[0], kernel_size=kernel_out[0], stride=out_stride[0], padding=in_trans_padding[0], output_padding=out_padding[0])\n",
    "        self.decBn2 = nn.BatchNorm2d(layers_out[0])\n",
    "        self.decConv2 = nn.ConvTranspose2d(in_channels=layers_out[0], out_channels=layers_out[1], kernel_size=kernel_out[1], stride=out_stride[1], padding=in_trans_padding[1], output_padding=out_padding[1])\n",
    "        self.decBn3 = nn.BatchNorm2d(layers_out[1])\n",
    "        self.decConv3 = nn.ConvTranspose2d(in_channels=layers_out[1], out_channels=layers_out[2], kernel_size=kernel_out[2], stride=out_stride[2], padding=in_trans_padding[2], output_padding=out_padding[2])\n",
    "        self.decBn4 = nn.BatchNorm2d(layers_out[2])\n",
    "        self.decConv4 = nn.ConvTranspose2d(in_channels=layers_out[2], out_channels=layers_out[3], kernel_size=kernel_out[3], stride=out_stride[3], padding=in_trans_padding[3], output_padding=out_padding[3])\n",
    "        self.decBn5 = nn.BatchNorm2d(layers_out[3])\n",
    "        self.decConv5 = nn.ConvTranspose2d(in_channels=layers_out[3], out_channels=imgChannels, kernel_size=kernel_out[4], stride=out_stride[4], padding=in_trans_padding[4], output_padding=out_padding[4])\n",
    "#         self.decBn6 = nn.BatchNorm2d(layers[1])\n",
    "#         self.decConv6 = nn.ConvTranspose2d(in_channels=layers[1], out_channels=layers[0], kernel_size=kernel[1], stride=stride[1], padding=in_trans_padding[5], output_padding=out_padding[5])\n",
    "#         self.decBn7 = nn.BatchNorm2d(layers[0])\n",
    "#         self.decConv7 = nn.ConvTranspose2d(in_channels=layers[0], out_channels=imgChannels, kernel_size=kernel[0], stride=stride[0], padding=in_trans_padding[6], output_padding=out_padding[6])\n",
    "        \n",
    "        self.final_encoder_dim = None\n",
    "        \n",
    "        decoderDims = self.calcDecoderDims(len(layers), encoderDims[-1], kernel_out, in_trans_padding, out_padding, out_stride)\n",
    "        self.printModel(layers, layers_out, encoderDims, decoderDims, imgSize, imgChannels)\n",
    "\n",
    "    def calcEncoderDims(self, layer_size, imageSize, kernel, in_padding, stride):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "#             tmpSize = int((newDims[-1]-kernel[x]+2*in_padding[x])/stride[x])+1\n",
    "            tmpSize = int(((newDims[-1] + 2*in_padding[x]-(kernel[x]-1)-1)/stride[x])+1)\n",
    "            newDims.append(tmpSize)\n",
    "        newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    def calcDecoderDims(self, layer_size, imageSize, kernel, in_trans_padding, out_padding, stride, d=1):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):            \n",
    "            tmpSize = (newDims[-1] - 1)*stride[x] - 2*in_trans_padding[x] + d*(kernel[x] - 1) + out_padding[x] + 1\n",
    "            newDims.append(tmpSize)\n",
    "#         newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    \n",
    "    def printModel(self, layers, layers_out, encDims, decDims, imageSize, imgChannels):\n",
    "        print(\"=============\")\n",
    "        print(\"Image Flow:\")\n",
    "        print(\"Encoder:\")\n",
    "        print(f\"{imageSize}x{imageSize}x{imgChannels} (Input Image)\")\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{encDims[x]}x{encDims[x]}x{layers[x]}\")\n",
    "        \n",
    "        print(\"Decoder:\")\n",
    "        for x in range(len(layers_out)):\n",
    "            if x == 0:\n",
    "                print(f\"{decDims[x]}x{decDims[x]}x{layers[-1]}\")\n",
    "            print(f\"{decDims[x]}x{decDims[x]}x{layers_out[x]}\")\n",
    "        print(f\"{decDims[-1]}x{decDims[-1]}x{imgChannels} (Output Image)\")\n",
    "        print(\"=============\")\n",
    "            \n",
    "        \n",
    "    def encoder(self, x):\n",
    "#         a = \n",
    "# #         b = self.res_conv1(x)\n",
    "#         print(a.size())\n",
    "#         x = x.resize_(2,32,510,510)\n",
    "#         print(x.size())\n",
    "        x1 = F.relu(self.encConv1(x))\n",
    "        x1 = self.encBn1(x1)\n",
    "#         x = self.res_conv1(x).resize_(parameters[\"batch_size\"],512,512)\n",
    "        x2 = F.relu(self.encConv2(x1))\n",
    "        x2 = self.encBn2(x2)\n",
    "        x3 = F.relu(self.encConv3(x2))\n",
    "        x3 = self.encBn3(x3)\n",
    "        x4 = F.relu(self.encConv4(x3))\n",
    "        x4 = self.encBn4(x4)\n",
    "        x5 = F.relu(self.encConv5(x4))\n",
    "        x5 = self.encBn5(x5)\n",
    "#         x6 = F.relu(self.encConv6(x5))\n",
    "#         x6 = self.encBn6(x6)\n",
    "#         x7 = F.relu(self.encConv7(x6))\n",
    "#         x7 = self.encBn7(x7)\n",
    "#         self.final_encoder_dim = np.array([x5.size(1), x5.size(2), x5.size(3)])\n",
    "#         flatten = np.prod(self.final_encoder_dim)\n",
    "\n",
    "#         x7 = x5.view(-1, flatten)\n",
    "#         z = self.encFC1(x7)\n",
    "        \n",
    "#         return z\n",
    "        return x5\n",
    "\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "#         d1 = F.relu(self.decFC1(z))\n",
    "#         d1 = self.decBn1(d1)\n",
    "#         d1 = d1.view(-1, self.final_encoder_dim[0], self.final_encoder_dim[1], self.final_encoder_dim[2])\n",
    "        d2 = F.relu(self.decConv1(z))\n",
    "        d2 = self.decBn2(d2)\n",
    "        d3 = F.relu(self.decConv2(d2))\n",
    "        d3 = self.decBn3(d3)\n",
    "        d4 = F.relu(self.decConv3(d3))\n",
    "        d4 = self.decBn4(d4)\n",
    "        d5 = F.relu(self.decConv4(d4))\n",
    "        d5 = self.decBn5(d5)\n",
    "#         d6 = F.relu(self.decConv5(d5))\n",
    "#         d6 = self.decBn6(d6)\n",
    "#         d7 = F.relu(self.decConv6(d6))\n",
    "#         d7 = self.decBn7(d7)\n",
    "        d8 = torch.sigmoid(self.decConv5(d5))\n",
    "        return d8\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "    \n",
    "#     def residual(self, x, out_channels, stride=2, kernel=1, padding=1):\n",
    "#         conv = nn.Conv2d(in_channels=imgChannels, out_channels=out_channels, kernel_size=kernel, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535b61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Image Flow:\n",
      "Encoder:\n",
      "512x512x3 (Input Image)\n",
      "512x512x64\n",
      "255x255x120\n",
      "127x127x240\n",
      "63x63x480\n",
      "31x31x512\n",
      "Decoder:\n",
      "31x31x64\n",
      "31x31x512\n",
      "63x63x256\n",
      "127x127x128\n",
      "255x255x64\n",
      "512x512x3 (Output Image)\n",
      "=============\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tsize mismatch for decConv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decConv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decConv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE()\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/disk/no_backup/is789/.Envs/Anomaly/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tsize mismatch for decConv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for decConv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decBn2.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decConv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3])."
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, recon_x):\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024), reduction = \"sum\")\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32 *3\n",
    "#     KLD_loss = 1 + log_var - mu.pow(2) - log_var.exp()\n",
    "#     KLD_loss = torch.sum(KLD_loss)\n",
    "#     KLD_loss *= -0.5\n",
    "#     return torch.mean(Recon_loss + KLD_loss)\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 2500), x.view(-1, 2500), reduction = \"sum\") * 32 * 32 *3\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\") * imgSize * imgSize *3\n",
    "    imgSize = parameters[\"imgSize\"]\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\")\n",
    "    return Recon_loss, Recon_loss\n",
    "#     return Recon_loss_adapted, Recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137499f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start carla simulatr beforehand\n",
    "sampler = Sampler(s_width=512, s_height=512, cam_height=4, cam_zoom=50, cam_rotation=-18)\n",
    "video = sampler.create_model_video(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = sampler.sample()\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# xx = np.array(img)\n",
    "# xx = (xx * 255).astype(\"int\")\n",
    "# cv2.imwrite(\"test.png\", xx)\n",
    "\n",
    "# kk = cv2.imread(\"test.png\")\n",
    "# image_rgb = cv2.cvtColor(kk, cv2.COLOR_BGR2RGB)\n",
    "# cv2.imwrite(\"test.png\", image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Sampler.load_Images(\"/disk/vanishing_data/is789/anomaly_samples/Samples_2022-07-12_10:44:49/\").astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_prediction(dataSet, index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs = torch.as_tensor(np.array([dataSet[index]]))\n",
    "        imgs = np.transpose(imgs, (0,3,2,1))\n",
    "        imgs = imgs.to(device)\n",
    "    #         img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        true_img = imgs[0].cpu().numpy()\n",
    "        true_img = np.transpose(true_img, (2,1,0))\n",
    "\n",
    "        out = model(imgs)\n",
    "    #         outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (2,1,0))\n",
    "        errorMatrix = np.absolute(true_img - out)\n",
    "        errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1] * errorMatrix.shape[2])\n",
    "        errorAvg = int(errorAvg * 100000)/ 100000.0\n",
    "\n",
    "        #plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax1.imshow(true_img)\n",
    "        ax2.set_title(f\"Reconstruction | MAE: {errorAvg}\")\n",
    "        ax2.imshow(out)\n",
    "        return fig\n",
    "    \n",
    "make_prediction(test_data, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly",
   "language": "python",
   "name": "anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
