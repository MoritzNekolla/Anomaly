{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c562d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "\n",
    "from Image_Sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"clearML\"\n",
    "PATH = \"model.pt\"\n",
    "# IMG_TRAIN = \"/disk/vanishing_data/is789/anomaly_samples/train_set/\"\n",
    "# IMG_TEST = \"/disk/vanishing_data/is789/anomaly_samples/40test/\"\n",
    "\n",
    "parameters = {\n",
    "    \"epoch\" : 16000,\n",
    "    \"batch_size\" : 10,\n",
    "    \"imgSize\": 512,\n",
    "    \"zDim\": 128,\n",
    "    \"learning_rate\" : 1e-05,\n",
    "    \"layers\" : [64, 128, 256, 256, 512, 512, 940],\n",
    "#     \"layers\" : [64, 120, 240, 480, 800],\n",
    "    \"reduce_threshold\" : [0.6,0.8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746b689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, imgChannels=3, imgSize=parameters[\"imgSize\"], zDim=parameters[\"zDim\"]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        \n",
    "        stride=[1,2,1,2,2,2,2]\n",
    "        out_stride=[2,2,2,2,2,2,2]\n",
    "#         in_stride=[1,2,2,2,2]\n",
    "#         out_stride=[1,2,2,2,1]\n",
    "        in_padding=[1,0,1,0,0,0,0]\n",
    "        in_trans_padding=[0,0,0,0,1,0,1]\n",
    "        out_padding=[0,0,0,0,0,1,0]\n",
    "        kernel=[3,3,3,3,3,3,3]\n",
    "#         layers=[128, 128, 128, 256, 256]\n",
    "        layers=parameters[\"layers\"]\n",
    "#         layers=[32, 64, 64, 128, 128]\n",
    "#         layers=[64, 128, 128, 128, 256]\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(in_channels=imgChannels, out_channels=layers[0], kernel_size=kernel[0], stride=stride[0], padding=in_padding[0])\n",
    "        self.encBn1 = nn.BatchNorm2d(layers[0])\n",
    "        self.encConv2 = nn.Conv2d(in_channels=layers[0], out_channels=layers[1], kernel_size=kernel[1], stride=stride[1], padding=in_padding[1])\n",
    "        self.encBn2 = nn.BatchNorm2d(layers[1])\n",
    "        self.encConv3 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2], kernel_size=kernel[2], stride=stride[2], padding=in_padding[2])\n",
    "        self.encBn3 = nn.BatchNorm2d(layers[2])\n",
    "        self.encConv4 = nn.Conv2d(in_channels=layers[2], out_channels=layers[3], kernel_size=kernel[3], stride=stride[3], padding=in_padding[3])\n",
    "        self.encBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.encConv5 = nn.Conv2d(in_channels=layers[3], out_channels=layers[4], kernel_size=kernel[4], stride=stride[4], padding=in_padding[4])\n",
    "        self.encBn5 = nn.BatchNorm2d(layers[4])\n",
    "        self.encConv6 = nn.Conv2d(in_channels=layers[4], out_channels=layers[5], kernel_size=kernel[5], stride=stride[5], padding=in_padding[5])\n",
    "        self.encBn6 = nn.BatchNorm2d(layers[5])\n",
    "        self.encConv7 = nn.Conv2d(in_channels=layers[5], out_channels=layers[6], kernel_size=kernel[6], stride=stride[6], padding=in_padding[6])\n",
    "        self.encBn7 = nn.BatchNorm2d(layers[6])\n",
    "        \n",
    "        encoderDims = self.calcEncoderDims(len(layers), imgSize, kernel, in_padding, stride)\n",
    "        featureDim = layers[-1] * encoderDims[-1] * encoderDims[-1]\n",
    "#         self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "#         self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "#         self.decBn1 = nn.BatchNorm1d(featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(in_channels=layers[6], out_channels=layers[5], kernel_size=kernel[6], stride=stride[6], padding=in_trans_padding[0], output_padding=out_padding[0])\n",
    "        self.decBn2 = nn.BatchNorm2d(layers[5])\n",
    "        self.decConv2 = nn.ConvTranspose2d(in_channels=layers[5], out_channels=layers[4], kernel_size=kernel[5], stride=stride[5], padding=in_trans_padding[1], output_padding=out_padding[1])\n",
    "        self.decBn3 = nn.BatchNorm2d(layers[4])\n",
    "        self.decConv3 = nn.ConvTranspose2d(in_channels=layers[4], out_channels=layers[3], kernel_size=kernel[4], stride=stride[4], padding=in_trans_padding[2], output_padding=out_padding[2])\n",
    "        self.decBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.decConv4 = nn.ConvTranspose2d(in_channels=layers[3], out_channels=layers[2], kernel_size=kernel[3], stride=stride[3], padding=in_trans_padding[3], output_padding=out_padding[3])\n",
    "        self.decBn5 = nn.BatchNorm2d(layers[2])\n",
    "        self.decConv5 = nn.ConvTranspose2d(in_channels=layers[2], out_channels=layers[1], kernel_size=kernel[2], stride=stride[2], padding=in_trans_padding[4], output_padding=out_padding[4])\n",
    "        self.decBn6 = nn.BatchNorm2d(layers[1])\n",
    "        self.decConv6 = nn.ConvTranspose2d(in_channels=layers[1], out_channels=layers[0], kernel_size=kernel[1], stride=stride[1], padding=in_trans_padding[5], output_padding=out_padding[5])\n",
    "        self.decBn7 = nn.BatchNorm2d(layers[0])\n",
    "        self.decConv7 = nn.ConvTranspose2d(in_channels=layers[0], out_channels=imgChannels, kernel_size=kernel[0], stride=stride[0], padding=in_trans_padding[6], output_padding=out_padding[6])\n",
    "        \n",
    "        self.final_encoder_dim = None\n",
    "        \n",
    "        decoderDims = self.calcDecoderDims(len(layers), encoderDims[-1], kernel, in_trans_padding, out_padding, stride)\n",
    "        self.printModel(layers, encoderDims, decoderDims, imgSize, imgChannels)\n",
    "\n",
    "    def calcEncoderDims(self, layer_size, imageSize, kernel, in_padding, stride):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "#             tmpSize = int((newDims[-1]-kernel[x]+2*in_padding[x])/stride[x])+1\n",
    "            tmpSize = int(((newDims[-1] + 2*in_padding[x]-(kernel[x]-1)-1)/stride[x])+1)\n",
    "            newDims.append(tmpSize)\n",
    "        newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    def calcDecoderDims(self, layer_size, imageSize, kernel, in_trans_padding, out_padding, stride, d=1):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):            \n",
    "            tmpSize = (newDims[-1] - 1)*stride[layer_size-1-x] - 2*in_trans_padding[x] + d*(kernel[layer_size-1-x] - 1) + out_padding[x] + 1\n",
    "            newDims.append(tmpSize)\n",
    "#         newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    \n",
    "    def printModel(self, layers, encDims, decDims, imageSize, imgChannels):\n",
    "        print(\"=============\")\n",
    "        print(\"Image Flow:\")\n",
    "        print(\"Encoder:\")\n",
    "        print(f\"{imageSize}x{imageSize}x{imgChannels} (Input Image)\")\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{encDims[x]}x{encDims[x]}x{layers[x]}\")\n",
    "        \n",
    "        print(\"Decoder:\")\n",
    "        k = len(layers) - 1\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{decDims[x]}x{decDims[x]}x{layers[k]}\")\n",
    "            k = k - 1\n",
    "        print(f\"{decDims[-1]}x{decDims[-1]}x{imgChannels} (Output Image)\")\n",
    "        print(\"=============\")\n",
    "            \n",
    "        \n",
    "    def encoder(self, x):\n",
    "\n",
    "        x = F.leaky_relu(self.encConv1(x))\n",
    "        x = self.encBn1(x)\n",
    "        x = F.leaky_relu(self.encConv2(x))\n",
    "        x = self.encBn2(x)\n",
    "        x = F.leaky_relu(self.encConv3(x))\n",
    "        x = self.encBn3(x)\n",
    "        x = F.leaky_relu(self.encConv4(x))\n",
    "        x = self.encBn4(x)\n",
    "        x = F.leaky_relu(self.encConv5(x))\n",
    "        x = self.encBn5(x)\n",
    "        x = F.leaky_relu(self.encConv6(x))\n",
    "        x = self.encBn6(x)\n",
    "        x = F.leaky_relu(self.encConv7(x))\n",
    "        x = self.encBn7(x)\n",
    "#         self.final_encoder_dim = np.array([x.size(1), x.size(2), x.size(3)])\n",
    "#         flatten = np.prod(self.final_encoder_dim)\n",
    "\n",
    "#         x = x.view(-1, flatten)\n",
    "#         z = F.leaky_relu(self.encFC1(x))\n",
    "        \n",
    "#         return z\n",
    "        return x\n",
    "\n",
    "#     def reparameterize(self, mu, logVar):\n",
    "\n",
    "#         #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "#         std = torch.exp(logVar/2)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + std * eps\n",
    "\n",
    "    def decoder(self, x):\n",
    "\n",
    "#         x = F.leaky_relu(self.decFC1(x))\n",
    "#         x = self.decBn1(x)\n",
    "#         x = x.view(-1, self.final_encoder_dim[0], self.final_encoder_dim[1], self.final_encoder_dim[2])\n",
    "        x = F.leaky_relu(self.decConv1(x))\n",
    "        x = self.decBn2(x)\n",
    "        x = F.leaky_relu(self.decConv2(x))\n",
    "        x = self.decBn3(x)\n",
    "        x = F.leaky_relu(self.decConv3(x))\n",
    "        x = self.decBn4(x)\n",
    "        x = F.leaky_relu(self.decConv4(x))\n",
    "        x = self.decBn5(x)\n",
    "        x = F.leaky_relu(self.decConv5(x))\n",
    "        x = self.decBn6(x)\n",
    "        x = F.leaky_relu(self.decConv6(x))\n",
    "        x = self.decBn7(x)\n",
    "        x = torch.sigmoid(self.decConv7(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535b61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Image Flow:\n",
      "Encoder:\n",
      "512x512x3 (Input Image)\n",
      "512x512x64\n",
      "255x255x128\n",
      "255x255x256\n",
      "127x127x256\n",
      "63x63x512\n",
      "31x31x512\n",
      "15x15x940\n",
      "Decoder:\n",
      "15x15x940\n",
      "31x31x512\n",
      "63x63x512\n",
      "127x127x256\n",
      "255x255x256\n",
      "255x255x128\n",
      "512x512x64\n",
      "512x512x3 (Output Image)\n",
      "=============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(PATH))\n",
      "File \u001b[0;32m/disk/no_backup/is789/.Envs/Anomaly/lib/python3.8/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/disk/no_backup/is789/.Envs/Anomaly/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/disk/no_backup/is789/.Envs/Anomaly/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/disk/no_backup/is789/.Envs/Anomaly/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, recon_x):\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024), reduction = \"sum\")\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32 *3\n",
    "#     KLD_loss = 1 + log_var - mu.pow(2) - log_var.exp()\n",
    "#     KLD_loss = torch.sum(KLD_loss)\n",
    "#     KLD_loss *= -0.5\n",
    "#     return torch.mean(Recon_loss + KLD_loss)\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 2500), x.view(-1, 2500), reduction = \"sum\") * 32 * 32 *3\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\") * imgSize * imgSize *3\n",
    "    imgSize = parameters[\"imgSize\"]\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\")\n",
    "    return Recon_loss, Recon_loss\n",
    "#     return Recon_loss_adapted, Recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start carla simulatr beforehand\n",
    "sampler = Sampler(s_width=512, s_height=512, cam_height=4, cam_zoom=50, cam_rotation=-18)\n",
    "video = sampler.create_model_video(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ea7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = sampler.sample()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a51d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# cv2.imwrite(\"test.png\", img)\n",
    "\n",
    "# plt.imshow(cv2.imread(\"test.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly",
   "language": "python",
   "name": "anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
