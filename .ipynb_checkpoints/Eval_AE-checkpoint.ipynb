{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c562d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "\n",
    "from Image_Sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"clearML\"\n",
    "PATH = \"model.pt\"\n",
    "# IMG_TRAIN = \"/disk/vanishing_data/is789/anomaly_samples/train_set/\"\n",
    "# IMG_TEST = \"/disk/vanishing_data/is789/anomaly_samples/40test/\"\n",
    "\n",
    "parameters = {\n",
    "    \"epoch\" : 16000,\n",
    "    \"batch_size\" : 10,\n",
    "    \"imgSize\": 512,\n",
    "    \"zDim\": 128,\n",
    "    \"learning_rate\" : 1e-05,\n",
    "#     \"layers\" : [64, 128, 256, 256, 512, 512, 940],\n",
    "    \"layers\" : [64, 120, 240, 480, 960],\n",
    "    \"reduce_threshold\" : [0.6,0.8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746b689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, imgChannels=3, imgSize=parameters[\"imgSize\"], zDim=parameters[\"zDim\"]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        \n",
    "        stride=[1,2,2,2,2]\n",
    "        out_stride=[2,2,2,2,1]\n",
    "#         in_stride=[1,2,2,2,2]\n",
    "#         out_stride=[1,2,2,2,1]\n",
    "        in_padding=[3,0,0,0,0]\n",
    "        in_trans_padding=[0,0,1,0,0]\n",
    "        out_padding=[0,0,1,0,0]\n",
    "        kernel=[7,3,3,3,3]\n",
    "        kernel_out=[3,3,4,4,1]\n",
    "#         layers=[128, 128, 128, 256, 256]\n",
    "        layers=parameters[\"layers\"]\n",
    "        layers_out = [32,32,32,32]\n",
    "#         layers=[32, 64, 64, 128, 128]\n",
    "#         layers=[64, 128, 128, 128, 256]\n",
    "        \n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(in_channels=imgChannels, out_channels=layers[0], kernel_size=kernel[0], stride=stride[0], padding=in_padding[0])\n",
    "        self.encBn1 = nn.BatchNorm2d(layers[0])\n",
    "        self.encConv2 = nn.Conv2d(in_channels=layers[0], out_channels=layers[1], kernel_size=kernel[1], stride=stride[1], padding=in_padding[1])\n",
    "        self.encBn2 = nn.BatchNorm2d(layers[1])\n",
    "        self.encConv3 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2], kernel_size=kernel[2], stride=stride[2], padding=in_padding[2])\n",
    "        self.encBn3 = nn.BatchNorm2d(layers[2])\n",
    "        self.encConv4 = nn.Conv2d(in_channels=layers[2], out_channels=layers[3], kernel_size=kernel[3], stride=stride[3], padding=in_padding[3])\n",
    "        self.encBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.encConv5 = nn.Conv2d(in_channels=layers[3], out_channels=layers[4], kernel_size=kernel[4], stride=stride[4], padding=in_padding[4])\n",
    "        self.encBn5 = nn.BatchNorm2d(layers[4])\n",
    "#         self.encConv6 = nn.Conv2d(in_channels=layers[4], out_channels=layers[5], kernel_size=kernel[5], stride=stride[5], padding=in_padding[5])\n",
    "#         self.encBn6 = nn.BatchNorm2d(layers[5])\n",
    "#         self.encConv7 = nn.Conv2d(in_channels=layers[5], out_channels=layers[6], kernel_size=kernel[6], stride=stride[6], padding=in_padding[6])\n",
    "#         self.encBn7 = nn.BatchNorm2d(layers[6])\n",
    "        \n",
    "        encoderDims = self.calcEncoderDims(len(layers), imgSize, kernel, in_padding, stride)\n",
    "#         featureDim = layers[-1] * encoderDims[-1] * encoderDims[-1]\n",
    "#         self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "# #         Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "#         self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "#         self.decBn1 = nn.BatchNorm1d(featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(in_channels=layers[4], out_channels=layers_out[0], kernel_size=kernel_out[0], stride=out_stride[0], padding=in_trans_padding[0], output_padding=out_padding[0])\n",
    "        self.decBn2 = nn.BatchNorm2d(layers_out[0])\n",
    "        self.decConv2 = nn.ConvTranspose2d(in_channels=layers_out[0], out_channels=layers_out[1], kernel_size=kernel_out[1], stride=out_stride[1], padding=in_trans_padding[1], output_padding=out_padding[1])\n",
    "        self.decBn3 = nn.BatchNorm2d(layers_out[1])\n",
    "        self.decConv3 = nn.ConvTranspose2d(in_channels=layers_out[1], out_channels=layers_out[2], kernel_size=kernel_out[2], stride=out_stride[2], padding=in_trans_padding[2], output_padding=out_padding[2])\n",
    "        self.decBn4 = nn.BatchNorm2d(layers_out[2])\n",
    "        self.decConv4 = nn.ConvTranspose2d(in_channels=layers_out[2], out_channels=layers_out[3], kernel_size=kernel_out[3], stride=out_stride[3], padding=in_trans_padding[3], output_padding=out_padding[3])\n",
    "        self.decBn5 = nn.BatchNorm2d(layers_out[3])\n",
    "        self.decConv5 = nn.ConvTranspose2d(in_channels=layers_out[3], out_channels=imgChannels, kernel_size=kernel_out[4], stride=out_stride[4], padding=in_trans_padding[4], output_padding=out_padding[4])\n",
    "#         self.decBn6 = nn.BatchNorm2d(layers[1])\n",
    "#         self.decConv6 = nn.ConvTranspose2d(in_channels=layers[1], out_channels=layers[0], kernel_size=kernel[1], stride=stride[1], padding=in_trans_padding[5], output_padding=out_padding[5])\n",
    "#         self.decBn7 = nn.BatchNorm2d(layers[0])\n",
    "#         self.decConv7 = nn.ConvTranspose2d(in_channels=layers[0], out_channels=imgChannels, kernel_size=kernel[0], stride=stride[0], padding=in_trans_padding[6], output_padding=out_padding[6])\n",
    "        \n",
    "        self.final_encoder_dim = None\n",
    "        \n",
    "        decoderDims = self.calcDecoderDims(len(layers), encoderDims[-1], kernel_out, in_trans_padding, out_padding, out_stride)\n",
    "        self.printModel(layers, layers_out, encoderDims, decoderDims, imgSize, imgChannels)\n",
    "\n",
    "    def calcEncoderDims(self, layer_size, imageSize, kernel, in_padding, stride):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "#             tmpSize = int((newDims[-1]-kernel[x]+2*in_padding[x])/stride[x])+1\n",
    "            tmpSize = int(((newDims[-1] + 2*in_padding[x]-(kernel[x]-1)-1)/stride[x])+1)\n",
    "            newDims.append(tmpSize)\n",
    "        newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    def calcDecoderDims(self, layer_size, imageSize, kernel, in_trans_padding, out_padding, stride, d=1):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):            \n",
    "            tmpSize = (newDims[-1] - 1)*stride[x] - 2*in_trans_padding[x] + d*(kernel[x] - 1) + out_padding[x] + 1\n",
    "            newDims.append(tmpSize)\n",
    "#         newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    \n",
    "    def printModel(self, layers, layers_out, encDims, decDims, imageSize, imgChannels):\n",
    "        print(\"=============\")\n",
    "        print(\"Image Flow:\")\n",
    "        print(\"Encoder:\")\n",
    "        print(f\"{imageSize}x{imageSize}x{imgChannels} (Input Image)\")\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{encDims[x]}x{encDims[x]}x{layers[x]}\")\n",
    "        \n",
    "        print(\"Decoder:\")\n",
    "        for x in range(len(layers_out)):\n",
    "            if x == 0:\n",
    "                print(f\"{decDims[x]}x{decDims[x]}x{layers[x]}\")\n",
    "            print(f\"{decDims[x]}x{decDims[x]}x{layers_out[x]}\")\n",
    "        print(f\"{decDims[-1]}x{decDims[-1]}x{imgChannels} (Output Image)\")\n",
    "        print(\"=============\")\n",
    "            \n",
    "        \n",
    "    def encoder(self, x):\n",
    "#         a = \n",
    "# #         b = self.res_conv1(x)\n",
    "#         print(a.size())\n",
    "#         x = x.resize_(2,32,510,510)\n",
    "#         print(x.size())\n",
    "        x1 = F.relu(self.encConv1(x))\n",
    "        x1 = self.encBn1(x1)\n",
    "#         x = self.res_conv1(x).resize_(parameters[\"batch_size\"],512,512)\n",
    "        x2 = F.relu(self.encConv2(x1))\n",
    "        x2 = self.encBn2(x2)\n",
    "        x3 = F.relu(self.encConv3(x2))\n",
    "        x3 = self.encBn3(x3)\n",
    "        x4 = F.relu(self.encConv4(x3))\n",
    "        x4 = self.encBn4(x4)\n",
    "        x5 = F.relu(self.encConv5(x4))\n",
    "        x5 = self.encBn5(x5)\n",
    "#         x6 = F.relu(self.encConv6(x5))\n",
    "#         x6 = self.encBn6(x6)\n",
    "#         x7 = F.relu(self.encConv7(x6))\n",
    "#         x7 = self.encBn7(x7)\n",
    "#         self.final_encoder_dim = np.array([x5.size(1), x5.size(2), x5.size(3)])\n",
    "#         flatten = np.prod(self.final_encoder_dim)\n",
    "\n",
    "#         x7 = x5.view(-1, flatten)\n",
    "#         z = self.encFC1(x7)\n",
    "        \n",
    "#         return z\n",
    "        return x5\n",
    "\n",
    "#     def reparameterize(self, mu, logVar):\n",
    "\n",
    "#         #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "#         std = torch.exp(logVar/2)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "#         d1 = F.relu(self.decFC1(z))\n",
    "#         d1 = self.decBn1(d1)\n",
    "#         d1 = d1.view(-1, self.final_encoder_dim[0], self.final_encoder_dim[1], self.final_encoder_dim[2])\n",
    "        d2 = F.relu(self.decConv1(z))\n",
    "        d2 = self.decBn2(d2)\n",
    "        d3 = F.relu(self.decConv2(d2))\n",
    "        d3 = self.decBn3(d3)\n",
    "        d4 = F.relu(self.decConv3(d3))\n",
    "        d4 = self.decBn4(d4)\n",
    "        d5 = F.relu(self.decConv4(d4))\n",
    "        d5 = self.decBn5(d5)\n",
    "#         d6 = F.relu(self.decConv5(d5))\n",
    "#         d6 = self.decBn6(d6)\n",
    "#         d7 = F.relu(self.decConv6(d6))\n",
    "#         d7 = self.decBn7(d7)\n",
    "        d8 = torch.sigmoid(self.decConv5(d5))\n",
    "        return d8\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "    \n",
    "#     def residual(self, x, out_channels, stride=2, kernel=1, padding=1):\n",
    "#         conv = nn.Conv2d(in_channels=imgChannels, out_channels=out_channels, kernel_size=kernel, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535b61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Image Flow:\n",
      "Encoder:\n",
      "512x512x3 (Input Image)\n",
      "512x512x64\n",
      "255x255x128\n",
      "255x255x256\n",
      "127x127x256\n",
      "63x63x512\n",
      "31x31x512\n",
      "15x15x940\n",
      "Decoder:\n",
      "15x15x940\n",
      "31x31x512\n",
      "63x63x512\n",
      "127x127x256\n",
      "255x255x256\n",
      "255x255x128\n",
      "512x512x64\n",
      "512x512x3 (Output Image)\n",
      "=============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891e8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, recon_x):\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024), reduction = \"sum\")\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32 *3\n",
    "#     KLD_loss = 1 + log_var - mu.pow(2) - log_var.exp()\n",
    "#     KLD_loss = torch.sum(KLD_loss)\n",
    "#     KLD_loss *= -0.5\n",
    "#     return torch.mean(Recon_loss + KLD_loss)\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 2500), x.view(-1, 2500), reduction = \"sum\") * 32 * 32 *3\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\") * imgSize * imgSize *3\n",
    "    imgSize = parameters[\"imgSize\"]\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\")\n",
    "    return Recon_loss, Recon_loss\n",
    "#     return Recon_loss_adapted, Recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d137499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start carla simulatr beforehand\n",
    "sampler = Sampler(s_width=512, s_height=512, cam_height=4, cam_zoom=50, cam_rotation=-18)\n",
    "video = sampler.create_model_video(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db3ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = sampler.sample()\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db40ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# xx = np.array(img)\n",
    "# xx = (xx * 255).astype(\"int\")\n",
    "# cv2.imwrite(\"test.png\", xx)\n",
    "\n",
    "# kk = cv2.imread(\"test.png\")\n",
    "# image_rgb = cv2.cvtColor(kk, cv2.COLOR_BGR2RGB)\n",
    "# cv2.imwrite(\"test.png\", image_rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anomaly",
   "language": "python",
   "name": "anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
